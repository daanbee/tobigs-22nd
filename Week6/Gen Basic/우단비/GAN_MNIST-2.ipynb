{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#1-1\n",
        "\n",
        "MNIST 데이터셋을 사용하여 간단한 GAN을 구현한 코드입니다.\n",
        "\n",
        "코드를 실행시키고, 주석을 달아주세요."
      ],
      "metadata": {
        "id": "PRlb757-wpKb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n"
      ],
      "metadata": {
        "id": "2a96QRuEwV-c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "batch_size = 100\n",
        "learning_rate = 0.0002\n",
        "img_size = 28 * 28\n",
        "noise_size = 100\n",
        "hidden_size1 = 256\n",
        "hidden_size2 = 512\n",
        "hidden_size3 = 1024\n",
        "dir_name = \"GAN_results\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if not os.path.exists(dir_name):\n",
        "    os.makedirs(dir_name)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # 이미지를 텐서로 변환\n",
        "    transforms.Normalize([0.5], [0.5]) # 이미지 정규화\n",
        "])\n",
        "\n",
        "# MNIST 데이터셋 로드\n",
        "MNIST_dataset = datasets.MNIST(root='../../data/',\n",
        "                               train=True,\n",
        "                               transform=transform,\n",
        "                               download=True)\n",
        "\n",
        "# DataLoader 설정\n",
        "data_loader = torch.utils.data.DataLoader(dataset=MNIST_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=True)"
      ],
      "metadata": {
        "id": "_D5vGGkTwXFQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc199b02-ba1b-4326-9d54-7b6ab9ba6998"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 20777716.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/train-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 628098.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 5654636.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2917832.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential( # sequential 모델 : 입력을 여러 계층에 순차적으로 전달\n",
        "            nn.Linear(img_size, hidden_size3),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_size3, hidden_size2),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_size2, hidden_size1),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(hidden_size1, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # 모델의 순전파\n",
        "        return self.model(x)\n",
        "\n",
        "class Generator(nn.Module): # 노이즈로부터 이미지를 생성하는 모델\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(noise_size, hidden_size1),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size1, hidden_size2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size2, hidden_size3),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size3, img_size),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x): # 순전파\n",
        "        return self.model(x)\n",
        "\n",
        "discriminator = Discriminator().to(device) # 모델 인스턴스 생성하고 이동\n",
        "generator = Generator().to(device)\n",
        "\n",
        "criterion = nn.BCELoss() # 손실함수 정의\n",
        "d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate) # 파라미터를 업데이트 할 옵티마이저 정의\n",
        "g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n"
      ],
      "metadata": {
        "id": "BzKto3wuv_Mk"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    for i, (images, _) in enumerate(data_loader):\n",
        "        real_labels = torch.ones(batch_size, 1).to(device) # 진짜 이미지에 대한 레이블 생성\n",
        "        fake_labels = torch.zeros(batch_size, 1).to(device) # 가짜 이미지에 대한 레이블 생성\n",
        "\n",
        "        real_images = images.reshape(batch_size, -1).to(device) # 이미지 크기 변경 후 저장\n",
        "\n",
        "\n",
        "        g_optimizer.zero_grad() # 기울기 초기화\n",
        "\n",
        "        z = torch.randn(batch_size, noise_size).to(device) # 랜덤한 노이즈 벡터 생성\n",
        "        fake_images = generator(z) # 노이지를 입력받아 가짜 이미지 생성\n",
        "\n",
        "        g_loss = criterion(discriminator(fake_images), real_labels) # 가짜 이미지를 실제라고 예측하도록 유도\n",
        "        g_loss.backward() # 제너레이터 손실에 대해 역전파\n",
        "        g_optimizer.step() # 업데이트\n",
        "\n",
        "        d_optimizer.zero_grad() # 기울기 초기화\n",
        "\n",
        "        z = torch.randn(batch_size, noise_size).to(device) # 다시 가짜 이미지 생성\n",
        "        fake_images = generator(z)\n",
        "\n",
        "        real_loss = criterion(discriminator(real_images), real_labels) # 진짜 이미지를 진짜로 예측하도록 학습\n",
        "        fake_loss = criterion(discriminator(fake_images.detach()), fake_labels) # 가짜 이미지를 가짜로 예측하도록 학습\n",
        "        d_loss = (real_loss + fake_loss) / 2 # 손실 평균을 discriminator 손실로 설정\n",
        "\n",
        "        d_loss.backward() # 손실에 대해 역전파\n",
        "        d_optimizer.step() # 업데이트\n",
        "\n",
        "        # 학습 진행상황 출력\n",
        "        if (i + 1) % 150 == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{num_epochs}], Step [{i + 1}/{len(data_loader)}], \"\n",
        "                  f\"D Loss: {d_loss.item():.4f}, G Loss: {g_loss.item():.4f}\")\n",
        "\n",
        "    save_image(fake_images.view(batch_size, 1, 28, 28),\n",
        "               os.path.join(dir_name, f'GAN_fake_image_{epoch + 1}.png'))\n",
        "\n",
        "    # discriminator가 진짜와 가짜 이미지를 얼마나 잘 구분하는지 평가\n",
        "    d_performance = discriminator(real_images).mean().item()\n",
        "    g_performance = discriminator(fake_images).mean().item()\n",
        "    print(f\"---------Epoch [{epoch + 1}/{num_epochs}] : D Performance: {d_performance:.2f}, G Performance: {g_performance:.2f}\")"
      ],
      "metadata": {
        "id": "VwZVMuXawdQx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2db11f69-ccc9-45ae-f3ca-1a4bd599c42d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100], Step [150/600], D Loss: 0.0430, G Loss: 3.1124\n",
            "Epoch [1/100], Step [300/600], D Loss: 0.0533, G Loss: 14.3516\n",
            "Epoch [1/100], Step [450/600], D Loss: 0.0133, G Loss: 6.7415\n",
            "Epoch [1/100], Step [600/600], D Loss: 0.0118, G Loss: 11.3472\n",
            "---------Epoch [1/100] : D Performance: 0.99, G Performance: 0.00\n",
            "Epoch [2/100], Step [150/600], D Loss: 0.0008, G Loss: 9.9950\n",
            "Epoch [2/100], Step [300/600], D Loss: 0.0017, G Loss: 6.5479\n",
            "Epoch [2/100], Step [450/600], D Loss: 0.0008, G Loss: 11.2755\n",
            "Epoch [2/100], Step [600/600], D Loss: 0.0649, G Loss: 17.5674\n",
            "---------Epoch [2/100] : D Performance: 0.97, G Performance: 0.01\n",
            "Epoch [3/100], Step [150/600], D Loss: 0.0471, G Loss: 11.3675\n",
            "Epoch [3/100], Step [300/600], D Loss: 0.2561, G Loss: 5.0783\n",
            "Epoch [3/100], Step [450/600], D Loss: 0.8017, G Loss: 6.5744\n",
            "Epoch [3/100], Step [600/600], D Loss: 0.2068, G Loss: 10.9645\n",
            "---------Epoch [3/100] : D Performance: 0.89, G Performance: 0.04\n",
            "Epoch [4/100], Step [150/600], D Loss: 1.1512, G Loss: 2.4491\n",
            "Epoch [4/100], Step [300/600], D Loss: 0.4330, G Loss: 2.0399\n",
            "Epoch [4/100], Step [450/600], D Loss: 0.0882, G Loss: 3.2359\n",
            "Epoch [4/100], Step [600/600], D Loss: 0.2596, G Loss: 2.9525\n",
            "---------Epoch [4/100] : D Performance: 0.83, G Performance: 0.10\n",
            "Epoch [5/100], Step [150/600], D Loss: 0.0527, G Loss: 3.3003\n",
            "Epoch [5/100], Step [300/600], D Loss: 0.5380, G Loss: 2.4713\n",
            "Epoch [5/100], Step [450/600], D Loss: 2.7274, G Loss: 5.7002\n",
            "Epoch [5/100], Step [600/600], D Loss: 0.0349, G Loss: 7.2036\n",
            "---------Epoch [5/100] : D Performance: 0.97, G Performance: 0.01\n",
            "Epoch [6/100], Step [150/600], D Loss: 0.7678, G Loss: 1.5157\n",
            "Epoch [6/100], Step [300/600], D Loss: 0.4376, G Loss: 5.8184\n",
            "Epoch [6/100], Step [450/600], D Loss: 0.0810, G Loss: 2.9973\n",
            "Epoch [6/100], Step [600/600], D Loss: 0.6377, G Loss: 2.5205\n",
            "---------Epoch [6/100] : D Performance: 0.70, G Performance: 0.10\n",
            "Epoch [7/100], Step [150/600], D Loss: 0.2778, G Loss: 2.7475\n",
            "Epoch [7/100], Step [300/600], D Loss: 0.0948, G Loss: 2.9894\n",
            "Epoch [7/100], Step [450/600], D Loss: 0.1972, G Loss: 3.6324\n",
            "Epoch [7/100], Step [600/600], D Loss: 0.0916, G Loss: 5.2171\n",
            "---------Epoch [7/100] : D Performance: 0.91, G Performance: 0.00\n",
            "Epoch [8/100], Step [150/600], D Loss: 0.1215, G Loss: 7.6911\n",
            "Epoch [8/100], Step [300/600], D Loss: 0.1338, G Loss: 5.4179\n",
            "Epoch [8/100], Step [450/600], D Loss: 0.1154, G Loss: 4.1651\n",
            "Epoch [8/100], Step [600/600], D Loss: 0.0824, G Loss: 3.7581\n",
            "---------Epoch [8/100] : D Performance: 0.95, G Performance: 0.04\n",
            "Epoch [9/100], Step [150/600], D Loss: 0.0593, G Loss: 3.6337\n",
            "Epoch [9/100], Step [300/600], D Loss: 0.1115, G Loss: 6.6297\n",
            "Epoch [9/100], Step [450/600], D Loss: 0.0385, G Loss: 5.2139\n",
            "Epoch [9/100], Step [600/600], D Loss: 0.2216, G Loss: 6.8258\n",
            "---------Epoch [9/100] : D Performance: 0.89, G Performance: 0.00\n",
            "Epoch [10/100], Step [150/600], D Loss: 0.2282, G Loss: 6.9250\n",
            "Epoch [10/100], Step [300/600], D Loss: 0.1914, G Loss: 4.8098\n",
            "Epoch [10/100], Step [450/600], D Loss: 0.1834, G Loss: 3.5804\n",
            "Epoch [10/100], Step [600/600], D Loss: 0.2036, G Loss: 7.0607\n",
            "---------Epoch [10/100] : D Performance: 0.92, G Performance: 0.01\n",
            "Epoch [11/100], Step [150/600], D Loss: 0.0719, G Loss: 6.9016\n",
            "Epoch [11/100], Step [300/600], D Loss: 0.0457, G Loss: 7.6167\n",
            "Epoch [11/100], Step [450/600], D Loss: 0.1881, G Loss: 9.4012\n",
            "Epoch [11/100], Step [600/600], D Loss: 0.0811, G Loss: 5.6992\n",
            "---------Epoch [11/100] : D Performance: 0.95, G Performance: 0.01\n",
            "Epoch [12/100], Step [150/600], D Loss: 0.0713, G Loss: 4.3823\n",
            "Epoch [12/100], Step [300/600], D Loss: 0.1611, G Loss: 3.8825\n",
            "Epoch [12/100], Step [450/600], D Loss: 0.1538, G Loss: 4.2815\n",
            "Epoch [12/100], Step [600/600], D Loss: 0.1603, G Loss: 4.3851\n",
            "---------Epoch [12/100] : D Performance: 0.93, G Performance: 0.06\n",
            "Epoch [13/100], Step [150/600], D Loss: 0.1557, G Loss: 6.0389\n",
            "Epoch [13/100], Step [300/600], D Loss: 0.1446, G Loss: 5.0452\n",
            "Epoch [13/100], Step [450/600], D Loss: 0.1571, G Loss: 3.4600\n",
            "Epoch [13/100], Step [600/600], D Loss: 0.1759, G Loss: 3.4481\n",
            "---------Epoch [13/100] : D Performance: 0.89, G Performance: 0.02\n",
            "Epoch [14/100], Step [150/600], D Loss: 0.1941, G Loss: 4.3363\n",
            "Epoch [14/100], Step [300/600], D Loss: 0.0944, G Loss: 3.2182\n",
            "Epoch [14/100], Step [450/600], D Loss: 0.1614, G Loss: 3.2875\n",
            "Epoch [14/100], Step [600/600], D Loss: 0.1164, G Loss: 6.5009\n",
            "---------Epoch [14/100] : D Performance: 0.86, G Performance: 0.00\n",
            "Epoch [15/100], Step [150/600], D Loss: 0.1577, G Loss: 3.8150\n",
            "Epoch [15/100], Step [300/600], D Loss: 0.0744, G Loss: 4.8093\n",
            "Epoch [15/100], Step [450/600], D Loss: 0.0548, G Loss: 4.9316\n",
            "Epoch [15/100], Step [600/600], D Loss: 0.2973, G Loss: 2.6556\n",
            "---------Epoch [15/100] : D Performance: 0.84, G Performance: 0.13\n",
            "Epoch [16/100], Step [150/600], D Loss: 0.2807, G Loss: 2.7930\n",
            "Epoch [16/100], Step [300/600], D Loss: 0.1094, G Loss: 4.1698\n",
            "Epoch [16/100], Step [450/600], D Loss: 0.0798, G Loss: 6.0558\n",
            "Epoch [16/100], Step [600/600], D Loss: 0.2215, G Loss: 4.1002\n",
            "---------Epoch [16/100] : D Performance: 0.94, G Performance: 0.22\n",
            "Epoch [17/100], Step [150/600], D Loss: 0.3642, G Loss: 6.5929\n",
            "Epoch [17/100], Step [300/600], D Loss: 0.2318, G Loss: 4.1651\n",
            "Epoch [17/100], Step [450/600], D Loss: 0.1426, G Loss: 3.4121\n",
            "Epoch [17/100], Step [600/600], D Loss: 0.0622, G Loss: 5.0318\n",
            "---------Epoch [17/100] : D Performance: 0.95, G Performance: 0.02\n",
            "Epoch [18/100], Step [150/600], D Loss: 0.2382, G Loss: 4.6672\n",
            "Epoch [18/100], Step [300/600], D Loss: 0.1187, G Loss: 2.8597\n",
            "Epoch [18/100], Step [450/600], D Loss: 0.1630, G Loss: 5.3527\n",
            "Epoch [18/100], Step [600/600], D Loss: 0.2585, G Loss: 3.9469\n",
            "---------Epoch [18/100] : D Performance: 0.88, G Performance: 0.10\n",
            "Epoch [19/100], Step [150/600], D Loss: 0.2725, G Loss: 5.4506\n",
            "Epoch [19/100], Step [300/600], D Loss: 0.2778, G Loss: 4.1058\n",
            "Epoch [19/100], Step [450/600], D Loss: 0.1011, G Loss: 4.6049\n",
            "Epoch [19/100], Step [600/600], D Loss: 0.1774, G Loss: 4.0831\n",
            "---------Epoch [19/100] : D Performance: 0.88, G Performance: 0.04\n",
            "Epoch [20/100], Step [150/600], D Loss: 0.1606, G Loss: 2.7398\n",
            "Epoch [20/100], Step [300/600], D Loss: 0.0944, G Loss: 4.6953\n",
            "Epoch [20/100], Step [450/600], D Loss: 0.1991, G Loss: 4.7726\n",
            "Epoch [20/100], Step [600/600], D Loss: 0.0908, G Loss: 4.0898\n",
            "---------Epoch [20/100] : D Performance: 0.93, G Performance: 0.04\n",
            "Epoch [21/100], Step [150/600], D Loss: 0.1029, G Loss: 4.8783\n",
            "Epoch [21/100], Step [300/600], D Loss: 0.1696, G Loss: 5.3880\n",
            "Epoch [21/100], Step [450/600], D Loss: 0.1421, G Loss: 3.6809\n",
            "Epoch [21/100], Step [600/600], D Loss: 0.1789, G Loss: 5.1441\n",
            "---------Epoch [21/100] : D Performance: 0.92, G Performance: 0.06\n",
            "Epoch [22/100], Step [150/600], D Loss: 0.1675, G Loss: 4.7635\n",
            "Epoch [22/100], Step [300/600], D Loss: 0.0933, G Loss: 3.1956\n",
            "Epoch [22/100], Step [450/600], D Loss: 0.2366, G Loss: 4.1205\n",
            "Epoch [22/100], Step [600/600], D Loss: 0.1293, G Loss: 4.1872\n",
            "---------Epoch [22/100] : D Performance: 0.90, G Performance: 0.03\n",
            "Epoch [23/100], Step [150/600], D Loss: 0.1520, G Loss: 3.7947\n",
            "Epoch [23/100], Step [300/600], D Loss: 0.1997, G Loss: 5.3639\n",
            "Epoch [23/100], Step [450/600], D Loss: 0.1046, G Loss: 3.3816\n",
            "Epoch [23/100], Step [600/600], D Loss: 0.2878, G Loss: 6.5247\n",
            "---------Epoch [23/100] : D Performance: 0.88, G Performance: 0.02\n",
            "Epoch [24/100], Step [150/600], D Loss: 0.0723, G Loss: 3.4058\n",
            "Epoch [24/100], Step [300/600], D Loss: 0.1287, G Loss: 3.6444\n",
            "Epoch [24/100], Step [450/600], D Loss: 0.0590, G Loss: 4.7478\n",
            "Epoch [24/100], Step [600/600], D Loss: 0.2214, G Loss: 3.8720\n",
            "---------Epoch [24/100] : D Performance: 0.84, G Performance: 0.02\n",
            "Epoch [25/100], Step [150/600], D Loss: 0.2842, G Loss: 4.2155\n",
            "Epoch [25/100], Step [300/600], D Loss: 0.1902, G Loss: 4.7367\n",
            "Epoch [25/100], Step [450/600], D Loss: 0.1451, G Loss: 4.2203\n",
            "Epoch [25/100], Step [600/600], D Loss: 0.1347, G Loss: 4.8641\n",
            "---------Epoch [25/100] : D Performance: 0.92, G Performance: 0.02\n",
            "Epoch [26/100], Step [150/600], D Loss: 0.1345, G Loss: 4.3705\n",
            "Epoch [26/100], Step [300/600], D Loss: 0.0580, G Loss: 6.1371\n",
            "Epoch [26/100], Step [450/600], D Loss: 0.1385, G Loss: 4.3904\n",
            "Epoch [26/100], Step [600/600], D Loss: 0.1448, G Loss: 3.4836\n",
            "---------Epoch [26/100] : D Performance: 0.94, G Performance: 0.11\n",
            "Epoch [27/100], Step [150/600], D Loss: 0.0996, G Loss: 3.4116\n",
            "Epoch [27/100], Step [300/600], D Loss: 0.1031, G Loss: 4.4451\n",
            "Epoch [27/100], Step [450/600], D Loss: 0.1582, G Loss: 3.3173\n",
            "Epoch [27/100], Step [600/600], D Loss: 0.1029, G Loss: 4.0584\n",
            "---------Epoch [27/100] : D Performance: 0.94, G Performance: 0.04\n",
            "Epoch [28/100], Step [150/600], D Loss: 0.1468, G Loss: 4.5351\n",
            "Epoch [28/100], Step [300/600], D Loss: 0.1580, G Loss: 4.8446\n",
            "Epoch [28/100], Step [450/600], D Loss: 0.1292, G Loss: 3.7437\n",
            "Epoch [28/100], Step [600/600], D Loss: 0.1166, G Loss: 3.4836\n",
            "---------Epoch [28/100] : D Performance: 0.97, G Performance: 0.12\n",
            "Epoch [29/100], Step [150/600], D Loss: 0.2373, G Loss: 3.4423\n",
            "Epoch [29/100], Step [300/600], D Loss: 0.1890, G Loss: 5.3876\n",
            "Epoch [29/100], Step [450/600], D Loss: 0.1342, G Loss: 3.4103\n",
            "Epoch [29/100], Step [600/600], D Loss: 0.1841, G Loss: 3.1547\n",
            "---------Epoch [29/100] : D Performance: 0.91, G Performance: 0.08\n",
            "Epoch [30/100], Step [150/600], D Loss: 0.1665, G Loss: 3.6218\n",
            "Epoch [30/100], Step [300/600], D Loss: 0.1494, G Loss: 3.3611\n",
            "Epoch [30/100], Step [450/600], D Loss: 0.1310, G Loss: 3.2274\n",
            "Epoch [30/100], Step [600/600], D Loss: 0.1991, G Loss: 3.5648\n",
            "---------Epoch [30/100] : D Performance: 0.91, G Performance: 0.10\n",
            "Epoch [31/100], Step [150/600], D Loss: 0.3412, G Loss: 4.4932\n",
            "Epoch [31/100], Step [300/600], D Loss: 0.2028, G Loss: 4.8655\n",
            "Epoch [31/100], Step [450/600], D Loss: 0.2724, G Loss: 4.0956\n",
            "Epoch [31/100], Step [600/600], D Loss: 0.0903, G Loss: 4.5304\n",
            "---------Epoch [31/100] : D Performance: 0.94, G Performance: 0.05\n",
            "Epoch [32/100], Step [150/600], D Loss: 0.2128, G Loss: 2.6152\n",
            "Epoch [32/100], Step [300/600], D Loss: 0.1968, G Loss: 4.0599\n",
            "Epoch [32/100], Step [450/600], D Loss: 0.1664, G Loss: 2.6796\n",
            "Epoch [32/100], Step [600/600], D Loss: 0.2689, G Loss: 3.8986\n",
            "---------Epoch [32/100] : D Performance: 0.81, G Performance: 0.06\n",
            "Epoch [33/100], Step [150/600], D Loss: 0.3082, G Loss: 4.6106\n",
            "Epoch [33/100], Step [300/600], D Loss: 0.2342, G Loss: 3.8972\n",
            "Epoch [33/100], Step [450/600], D Loss: 0.2415, G Loss: 2.8335\n",
            "Epoch [33/100], Step [600/600], D Loss: 0.1931, G Loss: 2.7431\n",
            "---------Epoch [33/100] : D Performance: 0.93, G Performance: 0.10\n",
            "Epoch [34/100], Step [150/600], D Loss: 0.1941, G Loss: 4.1821\n",
            "Epoch [34/100], Step [300/600], D Loss: 0.1765, G Loss: 3.1825\n",
            "Epoch [34/100], Step [450/600], D Loss: 0.1484, G Loss: 3.2621\n",
            "Epoch [34/100], Step [600/600], D Loss: 0.1467, G Loss: 3.4932\n",
            "---------Epoch [34/100] : D Performance: 0.86, G Performance: 0.05\n",
            "Epoch [35/100], Step [150/600], D Loss: 0.2275, G Loss: 3.0199\n",
            "Epoch [35/100], Step [300/600], D Loss: 0.1975, G Loss: 3.2481\n",
            "Epoch [35/100], Step [450/600], D Loss: 0.1851, G Loss: 3.2272\n",
            "Epoch [35/100], Step [600/600], D Loss: 0.2061, G Loss: 2.5277\n",
            "---------Epoch [35/100] : D Performance: 0.91, G Performance: 0.12\n",
            "Epoch [36/100], Step [150/600], D Loss: 0.1564, G Loss: 3.7356\n",
            "Epoch [36/100], Step [300/600], D Loss: 0.2252, G Loss: 2.1746\n",
            "Epoch [36/100], Step [450/600], D Loss: 0.2861, G Loss: 2.6431\n",
            "Epoch [36/100], Step [600/600], D Loss: 0.1910, G Loss: 2.8375\n",
            "---------Epoch [36/100] : D Performance: 0.88, G Performance: 0.10\n",
            "Epoch [37/100], Step [150/600], D Loss: 0.2621, G Loss: 3.1421\n",
            "Epoch [37/100], Step [300/600], D Loss: 0.2000, G Loss: 2.7656\n",
            "Epoch [37/100], Step [450/600], D Loss: 0.2856, G Loss: 3.5622\n",
            "Epoch [37/100], Step [600/600], D Loss: 0.2304, G Loss: 3.5778\n",
            "---------Epoch [37/100] : D Performance: 0.83, G Performance: 0.06\n",
            "Epoch [38/100], Step [150/600], D Loss: 0.2157, G Loss: 2.3927\n",
            "Epoch [38/100], Step [300/600], D Loss: 0.2103, G Loss: 3.3572\n",
            "Epoch [38/100], Step [450/600], D Loss: 0.2136, G Loss: 3.0527\n",
            "Epoch [38/100], Step [600/600], D Loss: 0.2374, G Loss: 2.8519\n",
            "---------Epoch [38/100] : D Performance: 0.88, G Performance: 0.12\n",
            "Epoch [39/100], Step [150/600], D Loss: 0.2029, G Loss: 3.2840\n",
            "Epoch [39/100], Step [300/600], D Loss: 0.1660, G Loss: 2.9864\n",
            "Epoch [39/100], Step [450/600], D Loss: 0.2357, G Loss: 2.5010\n",
            "Epoch [39/100], Step [600/600], D Loss: 0.1517, G Loss: 2.6571\n",
            "---------Epoch [39/100] : D Performance: 0.92, G Performance: 0.11\n",
            "Epoch [40/100], Step [150/600], D Loss: 0.3049, G Loss: 3.2631\n",
            "Epoch [40/100], Step [300/600], D Loss: 0.2108, G Loss: 2.9052\n",
            "Epoch [40/100], Step [450/600], D Loss: 0.2426, G Loss: 2.6196\n",
            "Epoch [40/100], Step [600/600], D Loss: 0.2519, G Loss: 2.5207\n",
            "---------Epoch [40/100] : D Performance: 0.86, G Performance: 0.14\n",
            "Epoch [41/100], Step [150/600], D Loss: 0.2053, G Loss: 2.7921\n",
            "Epoch [41/100], Step [300/600], D Loss: 0.2739, G Loss: 2.5001\n",
            "Epoch [41/100], Step [450/600], D Loss: 0.2451, G Loss: 2.7341\n",
            "Epoch [41/100], Step [600/600], D Loss: 0.3595, G Loss: 3.4833\n",
            "---------Epoch [41/100] : D Performance: 0.74, G Performance: 0.06\n",
            "Epoch [42/100], Step [150/600], D Loss: 0.1381, G Loss: 2.3232\n",
            "Epoch [42/100], Step [300/600], D Loss: 0.1763, G Loss: 2.7952\n",
            "Epoch [42/100], Step [450/600], D Loss: 0.2866, G Loss: 1.9368\n",
            "Epoch [42/100], Step [600/600], D Loss: 0.2855, G Loss: 2.1369\n",
            "---------Epoch [42/100] : D Performance: 0.86, G Performance: 0.24\n",
            "Epoch [43/100], Step [150/600], D Loss: 0.3950, G Loss: 3.6744\n",
            "Epoch [43/100], Step [300/600], D Loss: 0.1937, G Loss: 2.7966\n",
            "Epoch [43/100], Step [450/600], D Loss: 0.2159, G Loss: 3.1396\n",
            "Epoch [43/100], Step [600/600], D Loss: 0.2720, G Loss: 2.2305\n",
            "---------Epoch [43/100] : D Performance: 0.84, G Performance: 0.11\n",
            "Epoch [44/100], Step [150/600], D Loss: 0.2693, G Loss: 2.3337\n",
            "Epoch [44/100], Step [300/600], D Loss: 0.2536, G Loss: 2.8533\n",
            "Epoch [44/100], Step [450/600], D Loss: 0.1928, G Loss: 2.0909\n",
            "Epoch [44/100], Step [600/600], D Loss: 0.2931, G Loss: 2.4475\n",
            "---------Epoch [44/100] : D Performance: 0.82, G Performance: 0.17\n",
            "Epoch [45/100], Step [150/600], D Loss: 0.3129, G Loss: 3.1053\n",
            "Epoch [45/100], Step [300/600], D Loss: 0.3019, G Loss: 1.9912\n",
            "Epoch [45/100], Step [450/600], D Loss: 0.2373, G Loss: 2.3355\n",
            "Epoch [45/100], Step [600/600], D Loss: 0.3270, G Loss: 2.8115\n",
            "---------Epoch [45/100] : D Performance: 0.83, G Performance: 0.15\n",
            "Epoch [46/100], Step [150/600], D Loss: 0.1868, G Loss: 2.9206\n",
            "Epoch [46/100], Step [300/600], D Loss: 0.3897, G Loss: 2.1141\n",
            "Epoch [46/100], Step [450/600], D Loss: 0.3108, G Loss: 1.9604\n",
            "Epoch [46/100], Step [600/600], D Loss: 0.2917, G Loss: 2.0368\n",
            "---------Epoch [46/100] : D Performance: 0.86, G Performance: 0.18\n",
            "Epoch [47/100], Step [150/600], D Loss: 0.2648, G Loss: 2.8596\n",
            "Epoch [47/100], Step [300/600], D Loss: 0.3277, G Loss: 2.7493\n",
            "Epoch [47/100], Step [450/600], D Loss: 0.2737, G Loss: 1.9537\n",
            "Epoch [47/100], Step [600/600], D Loss: 0.3552, G Loss: 2.7874\n",
            "---------Epoch [47/100] : D Performance: 0.74, G Performance: 0.12\n",
            "Epoch [48/100], Step [150/600], D Loss: 0.2968, G Loss: 2.4162\n",
            "Epoch [48/100], Step [300/600], D Loss: 0.2756, G Loss: 2.5093\n",
            "Epoch [48/100], Step [450/600], D Loss: 0.2493, G Loss: 2.2912\n",
            "Epoch [48/100], Step [600/600], D Loss: 0.2832, G Loss: 2.3652\n",
            "---------Epoch [48/100] : D Performance: 0.84, G Performance: 0.16\n",
            "Epoch [49/100], Step [150/600], D Loss: 0.2883, G Loss: 2.3774\n",
            "Epoch [49/100], Step [300/600], D Loss: 0.2515, G Loss: 2.1143\n",
            "Epoch [49/100], Step [450/600], D Loss: 0.3070, G Loss: 2.2090\n",
            "Epoch [49/100], Step [600/600], D Loss: 0.2695, G Loss: 2.0916\n",
            "---------Epoch [49/100] : D Performance: 0.84, G Performance: 0.18\n",
            "Epoch [50/100], Step [150/600], D Loss: 0.4151, G Loss: 1.9060\n",
            "Epoch [50/100], Step [300/600], D Loss: 0.3169, G Loss: 2.2378\n",
            "Epoch [50/100], Step [450/600], D Loss: 0.2333, G Loss: 2.0125\n",
            "Epoch [50/100], Step [600/600], D Loss: 0.3105, G Loss: 2.3338\n",
            "---------Epoch [50/100] : D Performance: 0.86, G Performance: 0.24\n",
            "Epoch [51/100], Step [150/600], D Loss: 0.2531, G Loss: 2.4572\n",
            "Epoch [51/100], Step [300/600], D Loss: 0.3396, G Loss: 1.5554\n",
            "Epoch [51/100], Step [450/600], D Loss: 0.3906, G Loss: 2.0712\n",
            "Epoch [51/100], Step [600/600], D Loss: 0.3041, G Loss: 2.5159\n",
            "---------Epoch [51/100] : D Performance: 0.82, G Performance: 0.19\n",
            "Epoch [52/100], Step [150/600], D Loss: 0.3482, G Loss: 1.9851\n",
            "Epoch [52/100], Step [300/600], D Loss: 0.3838, G Loss: 2.4912\n",
            "Epoch [52/100], Step [450/600], D Loss: 0.3926, G Loss: 1.6133\n",
            "Epoch [52/100], Step [600/600], D Loss: 0.3623, G Loss: 2.3040\n",
            "---------Epoch [52/100] : D Performance: 0.77, G Performance: 0.17\n",
            "Epoch [53/100], Step [150/600], D Loss: 0.2255, G Loss: 2.4140\n",
            "Epoch [53/100], Step [300/600], D Loss: 0.3252, G Loss: 2.2038\n",
            "Epoch [53/100], Step [450/600], D Loss: 0.3831, G Loss: 2.0726\n",
            "Epoch [53/100], Step [600/600], D Loss: 0.2783, G Loss: 2.1366\n",
            "---------Epoch [53/100] : D Performance: 0.83, G Performance: 0.21\n",
            "Epoch [54/100], Step [150/600], D Loss: 0.3343, G Loss: 1.9359\n",
            "Epoch [54/100], Step [300/600], D Loss: 0.3750, G Loss: 2.2677\n",
            "Epoch [54/100], Step [450/600], D Loss: 0.3592, G Loss: 1.8472\n",
            "Epoch [54/100], Step [600/600], D Loss: 0.3068, G Loss: 1.9130\n",
            "---------Epoch [54/100] : D Performance: 0.79, G Performance: 0.15\n",
            "Epoch [55/100], Step [150/600], D Loss: 0.2608, G Loss: 2.3156\n",
            "Epoch [55/100], Step [300/600], D Loss: 0.2916, G Loss: 2.5613\n",
            "Epoch [55/100], Step [450/600], D Loss: 0.4027, G Loss: 1.8580\n",
            "Epoch [55/100], Step [600/600], D Loss: 0.2283, G Loss: 3.3400\n",
            "---------Epoch [55/100] : D Performance: 0.82, G Performance: 0.09\n",
            "Epoch [56/100], Step [150/600], D Loss: 0.2563, G Loss: 2.0235\n",
            "Epoch [56/100], Step [300/600], D Loss: 0.2592, G Loss: 2.4630\n",
            "Epoch [56/100], Step [450/600], D Loss: 0.3254, G Loss: 2.2774\n",
            "Epoch [56/100], Step [600/600], D Loss: 0.3430, G Loss: 1.9869\n",
            "---------Epoch [56/100] : D Performance: 0.77, G Performance: 0.17\n",
            "Epoch [57/100], Step [150/600], D Loss: 0.3657, G Loss: 2.5413\n",
            "Epoch [57/100], Step [300/600], D Loss: 0.3808, G Loss: 1.9625\n",
            "Epoch [57/100], Step [450/600], D Loss: 0.3136, G Loss: 2.2567\n",
            "Epoch [57/100], Step [600/600], D Loss: 0.3400, G Loss: 1.9797\n",
            "---------Epoch [57/100] : D Performance: 0.75, G Performance: 0.17\n",
            "Epoch [58/100], Step [150/600], D Loss: 0.3314, G Loss: 2.3415\n",
            "Epoch [58/100], Step [300/600], D Loss: 0.3209, G Loss: 1.9590\n",
            "Epoch [58/100], Step [450/600], D Loss: 0.3932, G Loss: 2.2692\n",
            "Epoch [58/100], Step [600/600], D Loss: 0.3176, G Loss: 1.4496\n",
            "---------Epoch [58/100] : D Performance: 0.85, G Performance: 0.24\n",
            "Epoch [59/100], Step [150/600], D Loss: 0.2782, G Loss: 1.8247\n",
            "Epoch [59/100], Step [300/600], D Loss: 0.3924, G Loss: 2.0874\n",
            "Epoch [59/100], Step [450/600], D Loss: 0.3689, G Loss: 2.5410\n",
            "Epoch [59/100], Step [600/600], D Loss: 0.3504, G Loss: 2.6345\n",
            "---------Epoch [59/100] : D Performance: 0.74, G Performance: 0.13\n",
            "Epoch [60/100], Step [150/600], D Loss: 0.3321, G Loss: 2.4080\n",
            "Epoch [60/100], Step [300/600], D Loss: 0.3479, G Loss: 2.1671\n",
            "Epoch [60/100], Step [450/600], D Loss: 0.4036, G Loss: 1.6701\n",
            "Epoch [60/100], Step [600/600], D Loss: 0.3910, G Loss: 1.6326\n",
            "---------Epoch [60/100] : D Performance: 0.76, G Performance: 0.19\n",
            "Epoch [61/100], Step [150/600], D Loss: 0.3434, G Loss: 1.9692\n",
            "Epoch [61/100], Step [300/600], D Loss: 0.3999, G Loss: 2.1019\n",
            "Epoch [61/100], Step [450/600], D Loss: 0.3494, G Loss: 2.1781\n",
            "Epoch [61/100], Step [600/600], D Loss: 0.3383, G Loss: 1.9418\n",
            "---------Epoch [61/100] : D Performance: 0.82, G Performance: 0.26\n",
            "Epoch [62/100], Step [150/600], D Loss: 0.3303, G Loss: 1.5341\n",
            "Epoch [62/100], Step [300/600], D Loss: 0.3354, G Loss: 2.0469\n",
            "Epoch [62/100], Step [450/600], D Loss: 0.3952, G Loss: 1.4690\n",
            "Epoch [62/100], Step [600/600], D Loss: 0.3447, G Loss: 1.6704\n",
            "---------Epoch [62/100] : D Performance: 0.79, G Performance: 0.21\n",
            "Epoch [63/100], Step [150/600], D Loss: 0.3377, G Loss: 1.8220\n",
            "Epoch [63/100], Step [300/600], D Loss: 0.4157, G Loss: 1.9185\n",
            "Epoch [63/100], Step [450/600], D Loss: 0.4756, G Loss: 2.1192\n",
            "Epoch [63/100], Step [600/600], D Loss: 0.4516, G Loss: 2.1121\n",
            "---------Epoch [63/100] : D Performance: 0.75, G Performance: 0.23\n",
            "Epoch [64/100], Step [150/600], D Loss: 0.3407, G Loss: 1.9637\n",
            "Epoch [64/100], Step [300/600], D Loss: 0.4647, G Loss: 2.2160\n",
            "Epoch [64/100], Step [450/600], D Loss: 0.3349, G Loss: 2.3241\n",
            "Epoch [64/100], Step [600/600], D Loss: 0.3582, G Loss: 2.2607\n",
            "---------Epoch [64/100] : D Performance: 0.79, G Performance: 0.23\n",
            "Epoch [65/100], Step [150/600], D Loss: 0.4162, G Loss: 1.2046\n",
            "Epoch [65/100], Step [300/600], D Loss: 0.3654, G Loss: 2.0789\n",
            "Epoch [65/100], Step [450/600], D Loss: 0.4408, G Loss: 1.6208\n",
            "Epoch [65/100], Step [600/600], D Loss: 0.4030, G Loss: 1.3742\n",
            "---------Epoch [65/100] : D Performance: 0.77, G Performance: 0.25\n",
            "Epoch [66/100], Step [150/600], D Loss: 0.4060, G Loss: 1.8285\n",
            "Epoch [66/100], Step [300/600], D Loss: 0.2685, G Loss: 3.1658\n",
            "Epoch [66/100], Step [450/600], D Loss: 0.4058, G Loss: 1.8728\n",
            "Epoch [66/100], Step [600/600], D Loss: 0.4392, G Loss: 1.5071\n",
            "---------Epoch [66/100] : D Performance: 0.77, G Performance: 0.29\n",
            "Epoch [67/100], Step [150/600], D Loss: 0.4088, G Loss: 1.3515\n",
            "Epoch [67/100], Step [300/600], D Loss: 0.4246, G Loss: 1.4421\n",
            "Epoch [67/100], Step [450/600], D Loss: 0.3744, G Loss: 2.0476\n",
            "Epoch [67/100], Step [600/600], D Loss: 0.3610, G Loss: 1.6533\n",
            "---------Epoch [67/100] : D Performance: 0.76, G Performance: 0.23\n",
            "Epoch [68/100], Step [150/600], D Loss: 0.3433, G Loss: 2.0986\n",
            "Epoch [68/100], Step [300/600], D Loss: 0.3731, G Loss: 1.7954\n",
            "Epoch [68/100], Step [450/600], D Loss: 0.3824, G Loss: 1.1506\n",
            "Epoch [68/100], Step [600/600], D Loss: 0.3786, G Loss: 1.4830\n",
            "---------Epoch [68/100] : D Performance: 0.75, G Performance: 0.24\n",
            "Epoch [69/100], Step [150/600], D Loss: 0.4167, G Loss: 1.5673\n",
            "Epoch [69/100], Step [300/600], D Loss: 0.4184, G Loss: 1.5756\n",
            "Epoch [69/100], Step [450/600], D Loss: 0.4467, G Loss: 2.0288\n",
            "Epoch [69/100], Step [600/600], D Loss: 0.4389, G Loss: 1.5939\n",
            "---------Epoch [69/100] : D Performance: 0.74, G Performance: 0.26\n",
            "Epoch [70/100], Step [150/600], D Loss: 0.3241, G Loss: 1.8360\n",
            "Epoch [70/100], Step [300/600], D Loss: 0.4504, G Loss: 2.2363\n",
            "Epoch [70/100], Step [450/600], D Loss: 0.3621, G Loss: 1.7849\n",
            "Epoch [70/100], Step [600/600], D Loss: 0.3658, G Loss: 1.8936\n",
            "---------Epoch [70/100] : D Performance: 0.74, G Performance: 0.22\n",
            "Epoch [71/100], Step [150/600], D Loss: 0.3478, G Loss: 1.8378\n",
            "Epoch [71/100], Step [300/600], D Loss: 0.4455, G Loss: 2.4906\n",
            "Epoch [71/100], Step [450/600], D Loss: 0.3459, G Loss: 1.8859\n",
            "Epoch [71/100], Step [600/600], D Loss: 0.3137, G Loss: 1.5322\n",
            "---------Epoch [71/100] : D Performance: 0.78, G Performance: 0.19\n",
            "Epoch [72/100], Step [150/600], D Loss: 0.2912, G Loss: 2.2275\n",
            "Epoch [72/100], Step [300/600], D Loss: 0.3296, G Loss: 1.8894\n",
            "Epoch [72/100], Step [450/600], D Loss: 0.4509, G Loss: 1.2293\n",
            "Epoch [72/100], Step [600/600], D Loss: 0.5051, G Loss: 1.4163\n",
            "---------Epoch [72/100] : D Performance: 0.72, G Performance: 0.30\n",
            "Epoch [73/100], Step [150/600], D Loss: 0.3872, G Loss: 1.6658\n",
            "Epoch [73/100], Step [300/600], D Loss: 0.3576, G Loss: 1.7516\n",
            "Epoch [73/100], Step [450/600], D Loss: 0.3098, G Loss: 1.9457\n",
            "Epoch [73/100], Step [600/600], D Loss: 0.4412, G Loss: 1.5681\n",
            "---------Epoch [73/100] : D Performance: 0.77, G Performance: 0.33\n",
            "Epoch [74/100], Step [150/600], D Loss: 0.4281, G Loss: 1.5303\n",
            "Epoch [74/100], Step [300/600], D Loss: 0.3744, G Loss: 1.3375\n",
            "Epoch [74/100], Step [450/600], D Loss: 0.4585, G Loss: 1.3482\n",
            "Epoch [74/100], Step [600/600], D Loss: 0.4082, G Loss: 1.2255\n",
            "---------Epoch [74/100] : D Performance: 0.77, G Performance: 0.31\n",
            "Epoch [75/100], Step [150/600], D Loss: 0.4359, G Loss: 1.5836\n",
            "Epoch [75/100], Step [300/600], D Loss: 0.3672, G Loss: 1.8201\n",
            "Epoch [75/100], Step [450/600], D Loss: 0.3988, G Loss: 2.0714\n",
            "Epoch [75/100], Step [600/600], D Loss: 0.4509, G Loss: 1.6700\n",
            "---------Epoch [75/100] : D Performance: 0.70, G Performance: 0.25\n",
            "Epoch [76/100], Step [150/600], D Loss: 0.4439, G Loss: 1.7999\n",
            "Epoch [76/100], Step [300/600], D Loss: 0.3608, G Loss: 1.7591\n",
            "Epoch [76/100], Step [450/600], D Loss: 0.3920, G Loss: 1.7164\n",
            "Epoch [76/100], Step [600/600], D Loss: 0.3790, G Loss: 1.6364\n",
            "---------Epoch [76/100] : D Performance: 0.74, G Performance: 0.22\n",
            "Epoch [77/100], Step [150/600], D Loss: 0.4486, G Loss: 1.5570\n",
            "Epoch [77/100], Step [300/600], D Loss: 0.4800, G Loss: 1.7457\n",
            "Epoch [77/100], Step [450/600], D Loss: 0.4606, G Loss: 1.9336\n",
            "Epoch [77/100], Step [600/600], D Loss: 0.4287, G Loss: 1.6614\n",
            "---------Epoch [77/100] : D Performance: 0.78, G Performance: 0.30\n",
            "Epoch [78/100], Step [150/600], D Loss: 0.4624, G Loss: 2.0496\n",
            "Epoch [78/100], Step [300/600], D Loss: 0.4101, G Loss: 1.8708\n",
            "Epoch [78/100], Step [450/600], D Loss: 0.4595, G Loss: 1.5057\n",
            "Epoch [78/100], Step [600/600], D Loss: 0.4744, G Loss: 1.8005\n",
            "---------Epoch [78/100] : D Performance: 0.68, G Performance: 0.24\n",
            "Epoch [79/100], Step [150/600], D Loss: 0.4271, G Loss: 1.5358\n",
            "Epoch [79/100], Step [300/600], D Loss: 0.4453, G Loss: 1.6607\n",
            "Epoch [79/100], Step [450/600], D Loss: 0.3961, G Loss: 1.6258\n",
            "Epoch [79/100], Step [600/600], D Loss: 0.4638, G Loss: 1.4722\n",
            "---------Epoch [79/100] : D Performance: 0.68, G Performance: 0.24\n",
            "Epoch [80/100], Step [150/600], D Loss: 0.4617, G Loss: 1.2687\n",
            "Epoch [80/100], Step [300/600], D Loss: 0.4179, G Loss: 1.6603\n",
            "Epoch [80/100], Step [450/600], D Loss: 0.4795, G Loss: 1.4339\n",
            "Epoch [80/100], Step [600/600], D Loss: 0.4224, G Loss: 1.5914\n",
            "---------Epoch [80/100] : D Performance: 0.71, G Performance: 0.23\n",
            "Epoch [81/100], Step [150/600], D Loss: 0.4004, G Loss: 1.7604\n",
            "Epoch [81/100], Step [300/600], D Loss: 0.3902, G Loss: 1.8761\n",
            "Epoch [81/100], Step [450/600], D Loss: 0.3832, G Loss: 1.5738\n",
            "Epoch [81/100], Step [600/600], D Loss: 0.4904, G Loss: 1.2222\n",
            "---------Epoch [81/100] : D Performance: 0.73, G Performance: 0.27\n",
            "Epoch [82/100], Step [150/600], D Loss: 0.4043, G Loss: 1.8581\n",
            "Epoch [82/100], Step [300/600], D Loss: 0.6006, G Loss: 1.1611\n",
            "Epoch [82/100], Step [450/600], D Loss: 0.5184, G Loss: 2.0318\n",
            "Epoch [82/100], Step [600/600], D Loss: 0.3706, G Loss: 1.4083\n",
            "---------Epoch [82/100] : D Performance: 0.82, G Performance: 0.32\n",
            "Epoch [83/100], Step [150/600], D Loss: 0.4746, G Loss: 1.3858\n",
            "Epoch [83/100], Step [300/600], D Loss: 0.4649, G Loss: 1.4188\n",
            "Epoch [83/100], Step [450/600], D Loss: 0.4989, G Loss: 1.9291\n",
            "Epoch [83/100], Step [600/600], D Loss: 0.5129, G Loss: 1.5885\n",
            "---------Epoch [83/100] : D Performance: 0.60, G Performance: 0.23\n",
            "Epoch [84/100], Step [150/600], D Loss: 0.4376, G Loss: 1.5951\n",
            "Epoch [84/100], Step [300/600], D Loss: 0.4366, G Loss: 1.5269\n",
            "Epoch [84/100], Step [450/600], D Loss: 0.4913, G Loss: 1.2783\n",
            "Epoch [84/100], Step [600/600], D Loss: 0.3979, G Loss: 1.5042\n",
            "---------Epoch [84/100] : D Performance: 0.74, G Performance: 0.24\n",
            "Epoch [85/100], Step [150/600], D Loss: 0.5483, G Loss: 1.4463\n",
            "Epoch [85/100], Step [300/600], D Loss: 0.4085, G Loss: 1.5454\n",
            "Epoch [85/100], Step [450/600], D Loss: 0.3825, G Loss: 1.7526\n",
            "Epoch [85/100], Step [600/600], D Loss: 0.3353, G Loss: 1.8020\n",
            "---------Epoch [85/100] : D Performance: 0.78, G Performance: 0.22\n",
            "Epoch [86/100], Step [150/600], D Loss: 0.4754, G Loss: 1.6185\n",
            "Epoch [86/100], Step [300/600], D Loss: 0.4228, G Loss: 1.6686\n",
            "Epoch [86/100], Step [450/600], D Loss: 0.4721, G Loss: 1.2718\n",
            "Epoch [86/100], Step [600/600], D Loss: 0.4689, G Loss: 1.3916\n",
            "---------Epoch [86/100] : D Performance: 0.75, G Performance: 0.33\n",
            "Epoch [87/100], Step [150/600], D Loss: 0.4330, G Loss: 1.6244\n",
            "Epoch [87/100], Step [300/600], D Loss: 0.4471, G Loss: 1.3171\n",
            "Epoch [87/100], Step [450/600], D Loss: 0.4561, G Loss: 1.4928\n",
            "Epoch [87/100], Step [600/600], D Loss: 0.5185, G Loss: 1.4354\n",
            "---------Epoch [87/100] : D Performance: 0.65, G Performance: 0.30\n",
            "Epoch [88/100], Step [150/600], D Loss: 0.4423, G Loss: 1.6303\n",
            "Epoch [88/100], Step [300/600], D Loss: 0.4130, G Loss: 1.5192\n",
            "Epoch [88/100], Step [450/600], D Loss: 0.4782, G Loss: 1.4017\n",
            "Epoch [88/100], Step [600/600], D Loss: 0.6363, G Loss: 1.3053\n",
            "---------Epoch [88/100] : D Performance: 0.59, G Performance: 0.30\n",
            "Epoch [89/100], Step [150/600], D Loss: 0.4307, G Loss: 1.3132\n",
            "Epoch [89/100], Step [300/600], D Loss: 0.3862, G Loss: 1.4033\n",
            "Epoch [89/100], Step [450/600], D Loss: 0.4395, G Loss: 1.5958\n",
            "Epoch [89/100], Step [600/600], D Loss: 0.4017, G Loss: 1.2588\n",
            "---------Epoch [89/100] : D Performance: 0.75, G Performance: 0.26\n",
            "Epoch [90/100], Step [150/600], D Loss: 0.3876, G Loss: 1.5405\n",
            "Epoch [90/100], Step [300/600], D Loss: 0.4678, G Loss: 1.6714\n",
            "Epoch [90/100], Step [450/600], D Loss: 0.4040, G Loss: 1.4262\n",
            "Epoch [90/100], Step [600/600], D Loss: 0.4269, G Loss: 1.3083\n",
            "---------Epoch [90/100] : D Performance: 0.70, G Performance: 0.26\n",
            "Epoch [91/100], Step [150/600], D Loss: 0.5288, G Loss: 1.3478\n",
            "Epoch [91/100], Step [300/600], D Loss: 0.4001, G Loss: 1.6596\n",
            "Epoch [91/100], Step [450/600], D Loss: 0.4223, G Loss: 1.5751\n",
            "Epoch [91/100], Step [600/600], D Loss: 0.5147, G Loss: 1.3130\n",
            "---------Epoch [91/100] : D Performance: 0.72, G Performance: 0.37\n",
            "Epoch [92/100], Step [150/600], D Loss: 0.5160, G Loss: 1.8706\n",
            "Epoch [92/100], Step [300/600], D Loss: 0.4856, G Loss: 1.4840\n",
            "Epoch [92/100], Step [450/600], D Loss: 0.4444, G Loss: 1.2310\n",
            "Epoch [92/100], Step [600/600], D Loss: 0.5239, G Loss: 1.7423\n",
            "---------Epoch [92/100] : D Performance: 0.67, G Performance: 0.30\n",
            "Epoch [93/100], Step [150/600], D Loss: 0.3747, G Loss: 1.7265\n",
            "Epoch [93/100], Step [300/600], D Loss: 0.4586, G Loss: 1.1721\n",
            "Epoch [93/100], Step [450/600], D Loss: 0.4334, G Loss: 1.7038\n",
            "Epoch [93/100], Step [600/600], D Loss: 0.4858, G Loss: 1.3668\n",
            "---------Epoch [93/100] : D Performance: 0.73, G Performance: 0.31\n",
            "Epoch [94/100], Step [150/600], D Loss: 0.5079, G Loss: 1.5079\n",
            "Epoch [94/100], Step [300/600], D Loss: 0.4576, G Loss: 1.0415\n",
            "Epoch [94/100], Step [450/600], D Loss: 0.3871, G Loss: 1.7389\n",
            "Epoch [94/100], Step [600/600], D Loss: 0.5065, G Loss: 1.7060\n",
            "---------Epoch [94/100] : D Performance: 0.66, G Performance: 0.25\n",
            "Epoch [95/100], Step [150/600], D Loss: 0.3868, G Loss: 1.4990\n",
            "Epoch [95/100], Step [300/600], D Loss: 0.5019, G Loss: 1.4181\n",
            "Epoch [95/100], Step [450/600], D Loss: 0.4508, G Loss: 1.4340\n",
            "Epoch [95/100], Step [600/600], D Loss: 0.4917, G Loss: 1.3551\n",
            "---------Epoch [95/100] : D Performance: 0.71, G Performance: 0.28\n",
            "Epoch [96/100], Step [150/600], D Loss: 0.4099, G Loss: 1.4357\n",
            "Epoch [96/100], Step [300/600], D Loss: 0.4823, G Loss: 1.6645\n",
            "Epoch [96/100], Step [450/600], D Loss: 0.4881, G Loss: 1.8563\n",
            "Epoch [96/100], Step [600/600], D Loss: 0.4256, G Loss: 1.4740\n",
            "---------Epoch [96/100] : D Performance: 0.74, G Performance: 0.31\n",
            "Epoch [97/100], Step [150/600], D Loss: 0.3408, G Loss: 1.7935\n",
            "Epoch [97/100], Step [300/600], D Loss: 0.3881, G Loss: 1.6976\n",
            "Epoch [97/100], Step [450/600], D Loss: 0.3813, G Loss: 1.8986\n",
            "Epoch [97/100], Step [600/600], D Loss: 0.4346, G Loss: 1.4569\n",
            "---------Epoch [97/100] : D Performance: 0.69, G Performance: 0.27\n",
            "Epoch [98/100], Step [150/600], D Loss: 0.3800, G Loss: 1.8201\n",
            "Epoch [98/100], Step [300/600], D Loss: 0.4341, G Loss: 1.5424\n",
            "Epoch [98/100], Step [450/600], D Loss: 0.4775, G Loss: 1.6043\n",
            "Epoch [98/100], Step [600/600], D Loss: 0.4527, G Loss: 1.6387\n",
            "---------Epoch [98/100] : D Performance: 0.65, G Performance: 0.22\n",
            "Epoch [99/100], Step [150/600], D Loss: 0.4419, G Loss: 1.5107\n",
            "Epoch [99/100], Step [300/600], D Loss: 0.4092, G Loss: 1.9090\n",
            "Epoch [99/100], Step [450/600], D Loss: 0.4221, G Loss: 1.8021\n",
            "Epoch [99/100], Step [600/600], D Loss: 0.5504, G Loss: 1.8633\n",
            "---------Epoch [99/100] : D Performance: 0.70, G Performance: 0.35\n",
            "Epoch [100/100], Step [150/600], D Loss: 0.4953, G Loss: 1.6336\n",
            "Epoch [100/100], Step [300/600], D Loss: 0.4391, G Loss: 1.6741\n",
            "Epoch [100/100], Step [450/600], D Loss: 0.4993, G Loss: 1.2799\n",
            "Epoch [100/100], Step [600/600], D Loss: 0.4346, G Loss: 1.6086\n",
            "---------Epoch [100/100] : D Performance: 0.74, G Performance: 0.32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1-2\n",
        "\n",
        "아래 마크다운으로 GAN_fake_image_1.png와 GAN_fake_image_100.png를 함께 첨부해주세요.\n",
        "\n",
        "![GAN_fake_image](GAN_fake_image_1.png)\n",
        "![GAN_fake_image](GAN_fake_image_100.png)"
      ],
      "metadata": {
        "id": "bYFHDcSxw8qo"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
